# -*- coding: utf-8 -*-
"""Data Lakehouse

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QsZu394GuN9aBa15yBSzZVDPW3_IcZ_M
"""

pip install delta-spark==2.1.0

#create session
import pyspark
from delta import *

builder = pyspark.sql.SparkSession.builder.appName("MyApp") \
.config("spark.jars.packages", "io.delta:delta-core_2.13:2.1.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

#only for testing. Ignore
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Michael","Rose","","5000","M",40000),
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)

df.drop('firstname').show()
df.dropDuplicates(['firstname', 'salary']).show()



#create table for device
#ignore
import datetime
from pyspark.sql.types import *
from pyspark.sql.functions import *
df=DeltaTable.create(spark) \
  .tableName("apple7") \
  .addColumn("device", "String") \
  .addColumn("timestamp", "String") \
  .addColumn("CO2_level", "long") \
  .addColumn("humidity", "long") \
  .addColumn("temperature", "long") \
  .addColumn("arrival_time", "TIMESTAMP") \
  .addColumn("monthOfBirth", DateType(), generatedAlwaysAs="CAST(arrival_time AS DATE)") \
  .partitionedBy("device","timestamp") \
  .location('/tmp/delta/apple7')\
  .execute()

#utility function
#read table from a file
Sampledata = spark.read.format("delta").load("/tmp/delta/apple8")
Sampledata.show()

# read from file
df2 = spark.read.json("./values1.json")
df2.write.mode(SaveMode.Overwrite).option("timestampFormat", "yyyy-MM-dd HH:mm")
#df2.coalesce(1).write.format("csv"
 #                           ).mode('overwrite'
  #                          ).option("header", "true"
  #                          ).option("timestampFormat", "yyyy-MM-dd HH:mm:ss"
  #                          ).save("date_fix.csv")

df2.printSchema()
df2.show()

from pyspark.sql import functions as F
df2=df2.withColumn("arrival_time",  F.current_timestamp())\
.withColumn("arrival_date", F.current_date())

#df2.write \
  #.format("delta") \
 # .mode("append") \
  #.save("/tmp/delta/apple8")

df2.withColumn("arrival_date",  F.current_date())

df2.withColumn("time_stamp_difference",  df2.arrival_date-to_date(df2.timestamp)).show()

spark.conf.set("spark.databricks.delta.replaceWhere.dataColumns.enabled", False)
spark.conf.set("spark.databricks.delta.replaceWhere.constraintCheck.enabled", False)

Sampledata.dropDuplicates(['device', 'timestamp']).show()

spark.sql("SELECT * FROM (SELECT *, ROW_NUMBER() OVER (Partition by device, timestamp ORDER BY timestamp) AS occurences  FROM apple6) tbl where tbl.occurences>1").show()

import pyspark.sql.functions.row_number

import os
os.path.abspath(os.getcwd())

"""# Third pipeline"""

#utility function
#read table from a cleansed data
#device_data = spark.read.format("delta").load("/tmp/delta/apple8")
device_data =spark.read.json("./values1.json")
device_data
device_info=spark.read.option("header",True).csv("./devices.csv")
merged_data=device_data.join(device_info,device_data["device"] == device_info["code"])
device_data.show()
device_info.show()
merged_data.show()

# takes as input the cleansed data: done
# joins the *device data* with the *device info*: done

# aggregates the data per month and area,
# and computes average values.
merged_data.show()
merged_data.registerTempTable("merged_data")
#aggregate by area
spark.sql("select AVG(CO2_level),AVG(humidity),AVG(temperature), area from merged_data group by area").show()
#aggregate by month
spark.sql("select AVG(CO2_level),AVG(humidity),AVG(temperature),month(timestamp) from merged_data group by month(timestamp)").show()
#store it and name the file based on the timestamp

# the code to try is here
#https://stackoverflow.com/questions/62290429/pyspark-output-to-csv-timestamp-format-is-different
df.coalesce(1).write.format("csv"
                            ).mode('overwrite'
                            ).option("header", "true"
                            ).option("timestampFormat", "yyyy-MM-dd HH:mm:ss"
                            ).save("date_fix.csv")

#this one changes the filename
import time
import os


# Getting the path of the file
f_path = os.getcwd()

# Obtaining the creation time (in seconds)
# of the file/folder (datatype=int)
t = os.path.getctime(f_path)

# Converting the time to an epoch string
# (the output timestamp string would
# be recognizable by strptime() without
# format quantifers)
t_str = time.ctime(t)

# Converting the string to a time object
t_obj = time.strptime(t_str)

# Transforming the time object to a timestamp
# of ISO 8601 format
form_t = time.strftime("%Y-%m-%d %H:%M:%S", t_obj)

# Since colon is an invalid character for a
# Windows file name Replacing colon with a
# similar looking symbol found in unicode
# Modified Letter Colon " " (U+A789)
form_t = form_t.replace(":", "êž‰")

# Renaming the filename to its timestamp
os.rename(
	f_path, os.path.split(f_path)[0] + '/' + form_t + os.path.splitext(f_path)[1])