# -*- coding: utf-8 -*-
"""Pipeline3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pR3CfpvZJW8h3MymFEOo67od1WlFr6qp
"""

#create session
import pyspark
from delta import *

builder = pyspark.sql.SparkSession.builder.appName("MyApp") \
.config("spark.jars.packages", "io.delta:delta-core_2.13:2.1.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

def getLatestFile(pathname):
  import glob
  import os
  list_of_files = glob.glob(pathname) # * means all if need specific format then *.csv
  import builtins
  return builtins.max(list_of_files,key=os.path.getmtime)

#directories
import sys
input_directory = sys.argv[1]
input_directory2= sys.argv[2]

from pyspark.sql import functions as F
import time
from pyspark.sql.functions import *
# ts stores the time in seconds
import datetime;
ct = datetime.datetime.today()
str_date_time = ct.strftime("%d_%m_%Y")
latest_file=getLatestFile(input_directory+'/*')
df_fromPipeline2 = spark.read.format("delta").load(latest_file)
latest_file2=getLatestFile(input_directory2+'/*')
device_info=spark.read.option("header",True).csv(latest_file2)
merged_data=df_fromPipeline2.join(device_info,df_fromPipeline2["device"] == device_info["code"])
merged_data.write \
  .format("delta") \
  .partitionBy("arrival_date") \
  .mode("append") \
  .save("./mergedData/"+str_date_time)

# takes as input the cleansed data: done
# joins the *device data* with the *device info*: done

# aggregates the data per month and area,
# and computes average values.
merged_data.show()
merged_data.registerTempTable("merged_data")
#aggregate by area
area=spark.sql("select AVG(CO2_level) as average_co2,AVG(humidity) as average_humidity,AVG(temperature) as average_temp, area from merged_data group by area")
area.write \
  .format("delta") \
  .mode("append") \
  .save("./area/"+str_date_time)

#aggregate by month
month=spark.sql("select AVG(CO2_level) as average_co2,AVG(humidity) as average_humidity,AVG(temperature) as average_temp,month(timestamp) as month from merged_data group by month(timestamp)")
#store it and name the file based on the timestamp
month.write \
  .format("delta") \
  .mode("append") \
  .save("./month/"+str_date_time)