# -*- coding: utf-8 -*-
"""Pipeline1_cleansed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UwyIB7RnJ67eiIYQR1CrgO7iNKyim8Ad
"""

#directories
import sys
input_directory = sys.argv[1]
output_directory= sys.argv[2]

#latest file
def getLatestFile(pathname):
  import glob
  import os
  list_of_files = glob.glob(pathname) # * means all if need specific format then *.csv
  import builtins
  return builtins.max(list_of_files,key=os.path.getmtime)

#create session
import pyspark
from delta import *

builder = pyspark.sql.SparkSession.builder.appName("MyApp") \
.config("spark.jars.packages", "io.delta:delta-core_2.13:2.1.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

#cleaning
from pyspark.sql import functions as F
import time
from pyspark.sql.functions import *
# ts stores the time in seconds
import datetime;
ct = datetime.datetime.today()
str_date_time = ct.strftime("%d_%m_%Y")
latest_file=getLatestFile(input_directory+'/*')
df_fromPipeline1 = spark.read.format("delta").load(latest_file)
df_fromPipeline1=df_fromPipeline1.withColumn("time_stamp_difference", datediff(df_fromPipeline1.arrival_date,to_date(df_fromPipeline1.timestamp)) )
df_fromPipeline1.show()
df_fromPipeline1=df_fromPipeline1.where(df_fromPipeline1.time_stamp_difference<2).dropDuplicates(['device', 'timestamp'])
df_fromPipeline1.write \
  .format("delta") \
  .partitionBy("arrival_date") \
  .mode("append") \
  .save(output_directory+'/'+str_date_time)
